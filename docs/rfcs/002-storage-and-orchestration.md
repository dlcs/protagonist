# Storage and Orchestration


## Context

From [What is the DLCS?](../what-is-dlcs-io.md)

> We could just have a cluster of one or more Image Servers and lots of disk space accessible by those servers.

> The problem with that is scale. For example, Wellcome has a lot of images: 40 million and counting, dozens of terabytes of disk space. They are already storing them in S3, for digital preservation.

> Access to these images also follows a steep long-tail distribution. Many images won't get looked at for months or even years, whereas some are looked at all the time.

> We want the best of both worlds - we want the low cost of S3 storage for terabytes of images, especially as many of them are rarely used. But we _need_ the performance and crucially the random access filesystem behaviour of EBS and similar volumes.

> The DLCS is an attempt to have this cake and eat it.

> The DLCS uses IIPImage under the hood (although it could use other image servers). But it fronts it with an _Orchestrator_ that copies files from S3 _origins_ to a local volume for use by IIPImage. As far as IIPImage is concerned, whenever it gets a request to extract a region from an image, that image is where it expects it to be on a locally readable disk. But it's only there because the orchestrator ensured it was there before the request arrived at IIPImage.

> At any one time, only a subset (e.g., 10%) of the possible images that the DLCS knows about (that have been registered) are on expensive fast disks. This _cache_ is what the DLCS maintains - ensuring images are present when IIPimage needs them, and scavenging disk space to keep this working set at a sensible size.

## Orchestration

The DLCS uses local EBS volumes for IIPImage to read, and the request pipeline holds up an image request if orchestration is required. It uses Redis and separate scavenger processes to maintain knowledge of what's where, and to keep the disk usage of this _hot cache_ stable.

But this is still our extra plumbing, more complexity to manage. 

**One ideal scenario is an imaginary AWS offering - S3-backed volumes where you can specify a source bucket, and the size of "real" volume you want (e.g., 1TB).**

The file system view can be read only - we don't need to write to the bucket via a filesystem, we can do that as S3. Reads of the filesystem manage the orchestration at that file access, below our application logic. We just assume that if it's in the bucket, it can be read from the file system, and everything looks simple.

This is _like_ S3fs-fuse, but a managed service. https://github.com/s3fs-fuse/s3fs-fuse

Other people use S3fs-fuse with IIPImage, but either with customisations - https://github.com/klokantech/embedr/issues/16 - or interventions for cache management that are similar to what we're already doing, so not particularly easier to manage.

Other approaches we looked at 5 years ago were commercial offerings on top of AWS, like SoftNAS, but they didn't have quite the features we wanted. Also file system for image servers based on GlusterFS.

There is an echo of AWS EFS in this. We tried using EFS rather than EBS for IIPImage, but found it too slow. A write operation is not finished until everything is consistent, and this was just too slow for image orchestration.


## Alternatives to Orchestration where possible

We're already offering the separate `/thumbs/` path for cases where the client knows what sizes to ask for.

We can take this further.

There's a difference between image requests for the `/full/` region, and tile requests. Full region requests (that don't match a thumbnail size) are likely to be the user looking at one image at a time, whereas tile requests arrive in a flood, for the same image, and are generated by deep zoom clients.

We could direct `/full/` requests down a different path, that doesn't orchestrate but makes byte range, or complete requests, to the source S3 JP2. We save orchestration space for tile requests, interactions that are triggered by deep zoom. Full region requests for small

We could even store the JP2 header as data, separately, so we don't need to go to S3 to read it. We have it handy so that if someone asks for a smallish size, but full region, we can read just enough to service it from the source JP2 with a byte range request. Someone asking for `/full/max/` can be made to wait longer than someone asking for a readable static image (perhaps the view before deep-zooming) or tiles, the kinds of user interactions are very different.

This discussion is related: https://groups.google.com/d/msg/iiif-discuss/OOkBKT8P3Y4/u2Lah-h_EAAJ

Serving tiles via small byte range requests to S3 still seems like a lot of work, I'd like IIPImage (or whatever we are using) to be dealing with as fast a file system as possible, as directly as possible, for handling tile requests. But we could end up where every other kind of `/full/` request is either handled by proxying a ready-made derivative in S3, or by on-the-fly image processing of a stream from S3.

Obviously, sensible reverse-proxy caching is important here too.

## Making orchestration somebody else's problem

### AWS File Gateway

On the face of it, AWS Storage Gateway looks a lot like the hypothetical service described earlier: https://aws.amazon.com/storagegateway/file/

The File Gateway can be run on EC2.

However, there are some issues that would limit us:

> An object that needs to be accessed by using a file share should only be managed by the gateway. If you directly overwrite or update an object previously written by file gateway, it results in undefined behavior when the object is accessed through the file share.

This would preclude use cases where the DLCS makes use of the existing buckets of an [archival storage system](https://github.com/wellcomecollection/docs/blob/extract-docs/rfcs/002-archival_storage/README.md); we'd need to copy images into another S3 bucket, which means synchronisation issues as well as huge amounts of extra storage.
 
(Update - while still here for reference, issues with AWS File Gateway are mitigated by the new [FSx File Gateway](https://aws.amazon.com/storagegateway/file/fsx/) - see below for more information.


### Azure Data Lake Storage

https://docs.microsoft.com/en-gb/azure/storage/blobs/data-lake-storage-introduction

> Azure Data Lake Storage Gen2 is a set of capabilities dedicated to big data analytics, built on Azure Blob storage. Data Lake Storage Gen2 is the result of converging the capabilities of our two existing storage services, Azure Blob storage and Azure Data Lake Storage Gen1. Features from Azure Data Lake Storage Gen1, such as file system semantics, directory, and file level security and scale are combined with low-cost, tiered storage, high availability/disaster recovery capabilities from Azure Blob storage.


### Amazon FSx for Lustre

https://aws.amazon.com/fsx/lustre/

Although this was available in 2018, the Lustre/S3 integration happened at setup time and wouldn't reflect subsequent changes in the bucket. This is no longer the case. 

From the FAQs:

> Q: If I have data in S3, how do I access it from Amazon FSx for Lustre?
> A: You can link your Amazon FSx for Lustre file system to your Amazon S3 bucket, and FSx will make your S3 data transparently accessible in your file system. Once your file system is created, the names and prefixes of objects in your S3 bucket will appear as file and directory listings on the file system. Although the names and prefixes of objects are listed as files and directories in your file system, the actual content of a given object is imported automatically from S3 only when you access the associated file on the file system for the first time – meaning an object’s data doesn’t consume space on your file system unless it’s accessed at least once on the file system.

And unlike the old File Gateway, the bucket continues to be a normal S3 bucket.

We need to be absolutely sure that the exposure of the bucket as a Lustre file system has no side effects in its use as a normal S3 bucket.
We would mount the file system as read only.

This seems to solve the problem of making Orchestration somebody else's problem:

1. Current DLCS - Orchestration is our problem and our code.
2. Alternatives like 3dfs - Orchestration is implemented with somebody else's code - better in _some_ ways
3. LustreFX - Orchestration is Amazon's problem, it's a managed service - best

Lustre can be 1.2 TB or increments of 2.4 TB. Something like Wellcome could have a 1.2 or 2.4 TB Lustre volume mounted on all the image servers. This volume is linked to the S3 bucket for Wellcome's optimised origin (storage bucket). All the existing and diverse interactions of other systems with that bucket continue as before, via S3 APIs (including DDS reading METS, text, etc). _Cross-account use of Lustre is not supported in AWS so we would need to setup a VPC Peering between store-account and dlcs-account, see https://docs.aws.amazon.com/fsx/latest/LustreGuide/mounting-on-premises.html_

Only image servers access files through Lustre, so only they fill up those 1.2 TB chunks. Even thumbs still uses the S3 version. An updated Orchestrator application still concerns itself with routing (including deciding to fulfill a request via S3 byte range, if necessary).

#### Managing the file system

The synchronisation between S3 and Lustre creates and maintains entries in the file system for everything in the bucket, but no _content_. That gets populated on demand. So we need to de-populate it, just as Echo-FS _scavenges_ the existing EBS volumes in DLCS, based on usage.

AWS have provided a sample for this:

https://github.com/aws-samples/fsx-solutions/blob/master/cache-eviction/readme.adoc

> ...the solution evicts (releases) space consumed by least recently accessed files on the FSx file system. 

The basic mechanism is this:

```python
  cmd = "sudo lfs hsm_release " + fileName
  p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True)
  (output,error) = p.communicate()
```

... where `lfs hsm_release` is [explained here](https://docs.aws.amazon.com/fsx/latest/LustreGuide/release-files.html)

We would likely not use that script as-is but base something on the existing Echo-FS logic.

#### Areas for investigation

 - Would we still want regular DLCS orchestration from other sources?
 - Ensuring S3 operations are not affected by the Lustre link
 - Do we even need to keep track of location information any more? (yes if we want to evict based on non-file-entry metrics)
 

## Next steps (for all options)

What are we missing here? What other ways of doing this are there? Is the system we've got actually the best way of doing it (with some modifications)?

Sources of concern:

A flood of tile requests for the same image can't all trigger orchestration of that image. We make it the equivalent of a critical section, we use a semaphore. While this is as light as possible, it still seems wasteful. Or at least, I'd rather it was someone else's problem. How does Lustre deal with this?

How well do the mentioned solutions handle multiple concurrent demands for the same file?

What's the most efficient way to optimise this? Avoiding multiple orchestration attempts, but recognising that all the request are independent? We use Redis and some Lua code in NGINX. New Orchestrator proposal puts this logic in C# under Kestrel (hopefully via YARP).
